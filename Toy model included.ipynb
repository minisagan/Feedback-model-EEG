{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "830837c8",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c38742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.chdir('C:\\\\Users\\\\User\\\\Desktop\\\\Internship')\n",
    "data_path = 'data.h5'\n",
    "data = h5py.File(data_path, 'r') \n",
    "import random\n",
    "\n",
    "\n",
    "N = 35 # length of lagged stim vector, how many past times we want it to depend on, N can be set to equal t if we want to consider all past values \n",
    "M = 35 #lenght of EEG feedback vector \n",
    "x = np.hstack([data[f'stim/part{0}'][:]])\n",
    "\n",
    "Y_t_allelectrodes = np.hstack([data[f'eeg/P00/part{0}'][:]]) #only looking at participant P00. but this is for all electrodes \n",
    "y = Y_t_allelectrodes[51] #update for speicific electrode  \n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "'''model parameters'''\n",
    "np.random.seed(0)\n",
    "k = np.random.normal(0, 0.01, N) \n",
    "\n",
    "np.random.seed(1)\n",
    "h = np.random.normal(0, 0.01, M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ee5ef8",
   "metadata": {},
   "source": [
    "# The model - definitions\n",
    "Lagged stimulus input vector: (N x 1) \\begin{align}\n",
    "    \\textbf{X} _{t} &= \\begin{bmatrix}\n",
    "           x_{t} \\\\\n",
    "           x_{t-1} \\\\\n",
    "           \\vdots \\\\\n",
    "           x_{t-N+1}\n",
    "         \\end{bmatrix}\n",
    "  \\end{align} \n",
    "\n",
    "\n",
    "EEG feedback vector (predicted EEG): (M x 1) \\begin{align}\n",
    "    \\mathbf{\\hat{Y}}_{t-1} &= \\begin{bmatrix}\n",
    "           \\hat{y}_{t-1} \\\\\n",
    "           \\vdots \\\\\n",
    "           \\hat{y}_{t-M}\n",
    "         \\end{bmatrix}\n",
    " \\end{align} \n",
    "  \n",
    "\n",
    "Model parameters: \\begin{align}\n",
    "     \\boldsymbol{k} = (k_1, k_2, ..., k_N)\\\\\n",
    "     \\boldsymbol{h} = (h_1, h_2, ..., h_M)\n",
    "\\end{align} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6235586f",
   "metadata": {},
   "source": [
    "# Getting  $\\textbf{X}_{t}$ and $\\mathbf{\\hat{Y}}_{t-1} $ and the toy model data\n",
    "\n",
    "y_hat: \\begin{align}\n",
    "\\hat{y}_{t}= \\boldsymbol{k} \\cdot \\mathbf{X}_t + \\boldsymbol{h} \\cdot \\mathbf{\\hat{Y}}_{t-1}\n",
    "\\end{align} \n",
    "\n",
    "y_toy: \\begin{align}\n",
    "{y}_{t}= \\boldsymbol{n} \\cdot \\mathbf{X}_t + \\boldsymbol{m} \\cdot \\mathbf{{Y}}_{t-1}\n",
    "\\end{align} \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4714d97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Generating the lagged stimulus vector'''\n",
    "\n",
    "def get_X(x, t, N): #t is the specific time and N is the legnth of the vector\n",
    "    X = np.zeros(N) #initialise X_t as a vector of zeros \n",
    "    if t >= N: \n",
    "        X = np.flip(x[t-N+1:t+1])  #if t>= N, we update X to contain the N elements preceding t from x\n",
    "    if t < N:\n",
    "        X[:t] = np.flip(x[:t])  #otherwise we extract the first t elements from x \n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "'''Generating the data for the toy model'''\n",
    "\n",
    "np.random.seed(2)\n",
    "n = np.random.normal(0, 0.1, N) \n",
    "np.random.seed(3)\n",
    "m = np.random.normal(0, 0.1, M)\n",
    "\n",
    "noise = np. random.normal(0, 0.01, x.shape)\n",
    "\n",
    "def generate_data(datasize, stimulus, m, n, noise): \n",
    "    Y_tmin1 = np.zeros(M)\n",
    "    Y = np.zeros(datasize)\n",
    "\n",
    "    for i in range(datasize):\n",
    "        y_i =0 \n",
    "        Y_tmin1 = np.roll(Y_tmin1, 1)\n",
    "        Y_tmin1[0]= y_i\n",
    "        X_t = get_X(x, i, N)\n",
    "        y_i = n @ X_t + m @ Y_tmin1\n",
    "        Y[i] = y_i\n",
    "    return Y + noise\n",
    "\n",
    "#y = generate_data(20271, x, m,n, noise)\n",
    "\n",
    "\n",
    "'''Generating predictions'''\n",
    "\n",
    "batchsize = 300\n",
    "iterate = int(np.ceil((2/3)*len(x)/batchsize))\n",
    "first = len(x)- batchsize*iterate\n",
    "\n",
    "def get_predictions(k, h, x, stop):\n",
    "    Y_hat = np.zeros(M)\n",
    "    predictions = np.zeros(stop)\n",
    "\n",
    "    for i in range(stop):\n",
    "        y_i =0 \n",
    "        Y_hat = np.roll(Y_hat, 1)\n",
    "        Y_hat[0]= y_i\n",
    "        X_t = get_X(x, i, N)\n",
    "        y_i = k @ X_t + h @ Y_hat \n",
    "        predictions[i] = y_i\n",
    "    return predictions\n",
    "\n",
    "predictions = get_predictions(k,h,x, 20271) \n",
    "\n",
    "\n",
    "def get_Y_hat(t):\n",
    "    Y_hat = np.zeros(M)\n",
    "    if t>= M: \n",
    "        Y_hat = np.flip(predictions[t-M : t])\n",
    "    if t<M: \n",
    "        Y_hat[:t] =  np.flip(predictions[ : t])\n",
    "        \n",
    "    return Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2104cd1d",
   "metadata": {},
   "source": [
    "# Getting errors \n",
    "\n",
    "get_sq_err_t: \\begin{align}\n",
    "\\epsilon_{t}= (\\hat{y}_{t}-y_{t})^{2} \n",
    "\\end{align}\n",
    "\n",
    "\n",
    "get_total_sq_err: \\begin{align}\n",
    "\\epsilon = \\sum_{t=start}^{stop}\\epsilon_{t} \n",
    "\\end{align}\n",
    "\n",
    "avg_batch_error:  \\begin{align}\n",
    "\\epsilon_{avg} = \\frac{\\epsilon}{batchsize}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043f4b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Getting the squared error at time t''' \n",
    "def get_sq_err_t(x, y, k, h, t): \n",
    "    Y_hat = get_Y_hat(t)\n",
    "    X = get_X(x, t, N)\n",
    "    y_hat = k @ X + h @ Y_hat\n",
    "    squared_error = (y_hat - y[t])**2\n",
    "    return squared_error \n",
    "\n",
    "'''summing to get total squared error'''\n",
    "def get_total_sq_err(x, y, k, h, start, stop):\n",
    "    total_errors = []\n",
    "    for t in range(start, stop): \n",
    "        squared_error = get_sq_err_t(x, y, k, h, t)\n",
    "        total_errors.append(squared_error)\n",
    "    total_sq_err = np.sum(total_errors)\n",
    "    return total_sq_err   \n",
    "\n",
    "'''average squared error per batch'''\n",
    "def avg_batch_error(x, y, k, h, start, stop, batchsize): \n",
    "    tot_error= get_total_sq_err(x, y, k, h, start, stop)\n",
    "    avg_batch_err = tot_error/batchsize\n",
    "    return(avg_batch_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd69268",
   "metadata": {},
   "source": [
    "# Derivative of error wrt k\n",
    "\n",
    "derivative_array_k:\n",
    "(M xN) \\begin{align}\n",
    "    \\frac{\\partial\\mathbf{\\hat{Y}}_{t-1}}{\\partial{\\boldsymbol{k}}} &= \\begin{bmatrix}\n",
    "           \\frac{\\partial{\\hat{y}_{t-1}}}{\\partial{\\boldsymbol{k}}} \\\\\n",
    "           \\vdots \\\\\n",
    "           \\frac{\\partial{\\hat{y}_{t-M}}}{\\partial{\\boldsymbol{k}}}\n",
    "         \\end{bmatrix}\n",
    " \\end{align} \n",
    "\n",
    "d_dk: (N x 1) \\begin{align}\n",
    "\\frac{\\partial{\\hat{y}_{t}}}{\\partial{k}} = \\mathbf{X}_t + \\boldsymbol{h} \\cdot \\frac {\\partial \\mathbf{\\hat{Y}}_{t-1}}{\\partial{\\boldsymbol{k}}}\n",
    "\\end{align} \n",
    "\n",
    "grad_times_error_k: \\begin{align}\n",
    "\\frac{\\partial{\\hat{y}_{t}}}{\\partial{\\boldsymbol{k}}} (\\hat{y}_{t} - y_t)\n",
    "\\end{align} \n",
    "\n",
    "error_deriv_wrt_k: \\begin{align}\n",
    "\\frac{\\partial{\\epsilon}}{\\partial{\\boldsymbol{k}}} =\n",
    "2 \\sum_{t=start}^{stop}\\frac{\\partial{\\epsilon_{t}}}{\\partial{\\boldsymbol{k}}}\n",
    "= 2 \\sum_{t=start}^{stop}\\frac{\\partial{\\hat{y}_{t}}}{\\partial{\\boldsymbol{k}}} (\\hat{y}_{t} - y_t)\n",
    "\\end{align} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf5f2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def err_derivative_k(x, y, t, k, h, start = 0, alpha_k = 1e-5):\n",
    "    derivative_array_k = np.zeros((M,N)) #array of zeroes to fill with the derivatives (d/dk(yhat_t-1),...,d/dk(yhat_t-M))\n",
    "    Y_hat = np.zeros(M) #array of zeroes to fill with the values of yhat_t-1,...,yhat_t-M\n",
    "    grad_times_error_array_k = np.zeros((t-start,N))\n",
    "    for i in range(start, t): \n",
    "        X = get_X(x, i, N)\n",
    "        \n",
    "        derivative_array_k = np.roll(derivative_array_k, 1, axis =0) #shifts all elements of the first row down one, and makes first row contain 0s\n",
    "        d_dk = X + h @ derivative_array_k #formula for the derivative of y_hat at time t, basically eliminitates k this will be a vector\n",
    "        derivative_array_k[0,:] = d_dk.T #replaces first row with derivatives \n",
    "        \n",
    "        y_t = y[i] \n",
    "        Y_hat = get_Y_hat(t)\n",
    "        y_hat = k @ X + h @ Y_hat \n",
    "    \n",
    "        grad_times_error_k = d_dk * (y_hat - y_t)\n",
    "        grad_times_error_array_k[i-start-1,:] = grad_times_error_k.T\n",
    "    error_deriv_wrt_k = 2 * np.sum(grad_times_error_array_k, axis = 0)\n",
    "        \n",
    "    return error_deriv_wrt_k + alpha_k*2*k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357a851d",
   "metadata": {},
   "source": [
    "# Derivative of error wrt h\n",
    "\n",
    "derivative_array_h:\n",
    "(MxM) \\begin{align}\n",
    "    \\frac{\\partial\\mathbf{\\hat{Y}}_{t-1}}{\\partial{\\boldsymbol{h}}} &= \\begin{bmatrix}\n",
    "           \\frac{\\partial{\\hat{y}_{t-1}}}{\\partial{\\boldsymbol{h}}} \\\\\n",
    "           \\vdots \\\\\n",
    "           \\frac{\\partial{\\hat{y}_{t-M}}}{\\partial{\\boldsymbol{h}}}\n",
    "         \\end{bmatrix}\n",
    " \\end{align} \n",
    "\n",
    "d_dh: (N x 1) \\begin{align}\n",
    "\\frac{\\partial{\\hat{y}_{t}}}{\\partial{\\boldsymbol{h}}} = \\mathbf{\\hat{Y}}_{t-1}+ \\boldsymbol{h} \\cdot \\frac {\\partial \\mathbf{\\hat{Y}}_{t-1}}{\\partial{\\boldsymbol{h}}}\n",
    "\\end{align} \n",
    "\n",
    "grad_times_error_h: \\begin{align}\n",
    "\\frac{\\partial{\\hat{y}_{t}}}{\\partial{\\boldsymbol{h}}} (\\hat{y}_{t} - y_t)\n",
    "\\end{align} \n",
    "\n",
    "error_deriv_wrt_h: \\begin{align}\n",
    "\\frac{\\partial{\\epsilon}}{\\partial{\\boldsymbol{h}}} =\n",
    "2 \\sum_{t=start}^{stop}\\frac{\\partial{\\epsilon_{t}}}{\\partial{\\boldsymbol{h}}}\n",
    "= 2 \\sum_{t=start}^{stop}\\frac{\\partial{\\hat{y}_{t}}}{\\partial{\\boldsymbol{h}}} (\\hat{y}_{t} - y_t)\n",
    "\\end{align} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de87e198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def err_derivative_h(x, y, t, k, h, start = 0, alpha_h= 1e-5): \n",
    "    derivative_array_h = np.zeros((M,M))\n",
    "    Y_hat = np.zeros(M)\n",
    "    grad_times_error_array_h = np.zeros((t-start,M))\n",
    "\n",
    "    for i in range(start, t):\n",
    "        Y_hat = get_Y_hat(t)\n",
    "        derivative_array_h = np.roll(derivative_array_h, 1, axis = 0)\n",
    "        d_dh = Y_hat + h @ derivative_array_h\n",
    "    \n",
    "        derivative_array_h[0,:] = d_dh.T\n",
    "        X = get_X(x, i, N)\n",
    "        y_t = y[i]\n",
    "        y_hat = k @ X + h @ Y_hat\n",
    "        \n",
    "        grad_times_error_h = d_dh * (y_hat - y_t)\n",
    "        grad_times_error_array_h[i-start-1,:] = grad_times_error_h.T\n",
    "    \n",
    "    error_deriv_wrt_h = 2 * np.sum(grad_times_error_array_h, axis = 0)\n",
    "    return error_deriv_wrt_h + alpha_h*2*h\n",
    "\n",
    "\n",
    "#err_derivative_h(40,k,h, start = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be0ade5",
   "metadata": {},
   "source": [
    "# Batch Descent\n",
    "Updating k and h: \n",
    "\\begin{align}\n",
    "\\boldsymbol{k}^{(i+1)}=\\boldsymbol{k}^{(i)} - \\lambda_k \\frac{\\partial{\\epsilon^{(i)}}}{\\partial{\\boldsymbol{k}}} \n",
    "\\end{align} \n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\boldsymbol{h}^{(i+1)}=\\boldsymbol{h}^{(i)} - \\lambda_h \\frac{\\partial{\\epsilon^{(i)}}}{\\partial{\\boldsymbol{h}}} \n",
    "\\end{align} \n",
    "\n",
    "avg_batch_error: \\begin{align}\n",
    "\\epsilon_{\\text{avg}}= \\frac{\\epsilon}{batchsize}\n",
    "\\end{align} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc5244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''simple batch descent'''\n",
    "\n",
    "def batchDescent(x, y, k, h, lambda_k, lambda_h, batchsize =300): #err_k = total_errors_k etc, 13514/233 = 58\n",
    "    MIN = 0\n",
    "    \n",
    "    iterate = int(np.ceil(len(x)/batchsize))\n",
    "                 \n",
    "    for j in range(iterate):\n",
    "        MIN = j*batchsize\n",
    "        if batchsize > x.size - MIN: #if the remainder of the train set < the batchsize, the final batch = remainder \n",
    "            batchsize = x.size%MIN\n",
    "        #start = time.time() \n",
    "        \n",
    "        \n",
    "        T = MIN + batchsize\n",
    "      \n",
    "        err_deriv_k = err_derivative_k(x, y, T, k, h, MIN)\n",
    "        err_deriv_h = err_derivative_h(x, y, T, k, h, MIN)\n",
    "        \n",
    "        k = k - lambda_k * err_deriv_k\n",
    "        h = h - lambda_h * err_deriv_h\n",
    "        \n",
    "        avg_batch_err = avg_batch_error(x, y, k, h, MIN, T, batchsize)\n",
    "        print(avg_batch_err)\n",
    "        \n",
    "        predictions = get_predictions(k,h,x, T) #update 20271 to be T\n",
    "        #print(pearsonr(predictions, y)) #update y to be y[MIN:T]\n",
    "              \n",
    "    return k, h\n",
    "\n",
    "\n",
    "k,h = batchDescent(x, y, k, h,1e-7,1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1783fd6f",
   "metadata": {},
   "source": [
    "# Plotting predictions vs actual data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fcc290",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = generate_data(20271, x, m, n, noise)\n",
    "predictions = get_predictions(k,h,x, 20271)\n",
    "times = range(20271)\n",
    "plt.plot(times,y)\n",
    "plt.plot(times, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ee3969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "pearsonr(predictions, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b0302d",
   "metadata": {},
   "source": [
    "# Alternative batch descents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953eb352",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''batch descent looped over training data many times'''\n",
    "\n",
    "def loopBatchDescent(x, y, k, h, lambda_k, lambda_h, batchsize =500): #err_k = total_errors_k etc, 13514/233 = 58\n",
    "    MIN = 0\n",
    "\n",
    "    for i in range(25):\n",
    "        iteration_error = []\n",
    "        iterations = int(np.ceil(len(x)/batchsize))  \n",
    "        #print(iterations)\n",
    "        for j in range(iterations):\n",
    "            MIN = j*batchsize \n",
    "            if batchsize > x.size - MIN: #if the remainder of the train set < the batchsize, the final batch = remainder \n",
    "                batchsize = x.size%MIN\n",
    "        \n",
    "            T = MIN + batchsize\n",
    "   \n",
    "            err_deriv_k = err_derivative_k(x, y, T, k, h, MIN)\n",
    "            err_deriv_h = err_derivative_h(x, y, T, k, h, MIN)\n",
    "        \n",
    "            k = k - lambda_k * err_deriv_k\n",
    "            h = h - lambda_h * err_deriv_h\n",
    "            \n",
    "            avg_batch_err = avg_batch_error(x, y, k, h, MIN, T, batchsize)\n",
    "            iteration_error.append(avg_batch_err)\n",
    "        predictions = get_predictions(k,h,x, 20271)\n",
    "        print(pearsonr(predictions, y))\n",
    "        \n",
    "        avg_iteration_error = np.sum(iteration_error)/iterations\n",
    "        print(avg_iteration_error)\n",
    "        \n",
    "    return k, h\n",
    "\n",
    "k,h = loopBatchDescent(x, y, k, h, 1e-6,1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fba203e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Nesterov momentum, looped batch descent'''\n",
    "\n",
    "def loopMomentumBatchDescent(x, y, k, h, lambda_k, lambda_h, gamma_k, gamma_h, batchsize =500): #err_k = total_errors_k etc, 13514/233 = 58\n",
    "    \n",
    "    for i in range(25):\n",
    "        iteration_error = []\n",
    "        \n",
    "        MIN = 0\n",
    "        update_h = 0 #(initialised)\n",
    "        update_k = 0\n",
    "        \n",
    "        iterations = int(np.ceil(len(x)/batchsize))\n",
    "         \n",
    "        #print(iterations)\n",
    "        for j in range(iterations):\n",
    "            MIN = j*batchsize\n",
    "            if batchsize > x.size - j*batchsize: #if the remainder of the train set < the batchsize, the final batch = remainder \n",
    "                batchsize = x.size%(j*batchsize)\n",
    "           \n",
    "            T = MIN + batchsize\n",
    "    \n",
    "            err_deriv_k = err_derivative_k(x, y, T, k, h, MIN)\n",
    "            err_deriv_h = err_derivative_h(x, y, T, k, h, MIN)\n",
    "        \n",
    "            update_k = gamma_k*update_k + lambda_k* err_deriv_k\n",
    "            k = k + gamma_k*update_k - lambda_k*err_deriv_k\n",
    "        \n",
    "            update_h = gamma_h*update_h + lambda_h* err_deriv_h\n",
    "            h = h +  gamma_h*update_h - lambda_h* err_deriv_h\n",
    "        \n",
    "            avg_batch_err = avg_batch_error(x, y, k, h, MIN, T, batchsize)\n",
    "            iteration_error.append(avg_batch_err)\n",
    "            \n",
    "        predictions = get_predictions(k,h,x, 20271)\n",
    "        #print(pearsonr(predictions, y))\n",
    "        \n",
    "        avg_iteration_error = np.sum(iteration_error)/iterations\n",
    "        print(avg_iteration_error)\n",
    "        \n",
    "    return k, h\n",
    "\n",
    "k,h = loopMomentumBatchDescent(x, y, k, h, 1e-7,1e-7, 0.5, 0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
